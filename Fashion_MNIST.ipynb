{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPxtIkJEEayJSgBASOgwCNx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karthikreddy1010/Deep-Learning_Basic_Projects/blob/main/Fashion_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrkWOyYZOsio",
        "outputId": "444cf888-c262-4c04-d65e-8a0fb15ed833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All images/graphs will be saved to: /content/outputs\n",
            "Device: cuda\n",
            "  GPU: Tesla T4\n",
            "  VRAM: 15.6 GB\n",
            "Setup complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:424: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  self.check_worker_number_rationality()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 60,000  |  Test: 10,000\n",
            "Training Evaluator CNN...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1398/3455737240.py:132: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == 'cuda'))\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:432: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  self.check_worker_number_rationality()\n",
            "/tmp/ipython-input-1398/3455737240.py:140: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(DEVICE.type == 'cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch   1  loss=0.3902  acc=85.7%\n",
            "  Epoch   5  loss=0.1570  acc=94.2%\n",
            "  Epoch  10  loss=0.0401  acc=98.5%\n",
            "  Epoch  15  loss=0.0035  acc=99.9%\n",
            "\n",
            "✓ Evaluator Test Accuracy: 94.31%\n",
            "Extracting real features for cFID baseline...\n",
            "Done.\n",
            "VAE parameters: 2,626,817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1398/3455737240.py:317: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler_vae = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == 'cuda'))\n",
            "/tmp/ipython-input-1398/3455737240.py:328: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(DEVICE.type == 'cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAE ep  1  total=210.122  recon=194.878  kl=15.245\n",
            "VAE ep  5  total=72.400  recon=57.265  kl=15.135\n",
            "VAE ep 10  total=62.802  recon=47.206  kl=15.596\n",
            "VAE ep 15  total=59.276  recon=43.292  kl=15.983\n",
            "VAE ep 20  total=57.319  recon=41.162  kl=16.157\n",
            "VAE ep 25  total=56.177  recon=39.936  kl=16.240\n",
            "VAE ep 30  total=55.781  recon=39.487  kl=16.294\n",
            "VAE training time: 8.5 min\n",
            "  [saved] ./outputs/vae_training_curves.png\n",
            "  [saved] ./outputs/vae_reconstructions.png\n",
            "  [saved] ./outputs/vae_samples.png\n",
            "  [saved] ./outputs/vae_interpolation.png\n",
            "Evaluating VAE...\n",
            "  5000 images  →  28577.4 imgs/s  (0.2s total)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1398/3455737240.py:187: LinAlgWarning: Matrix is singular. The result might be inaccurate or the array might not have a square root.\n",
            "  covmean = sqrtm(sig1 @ sig2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAE cFID:           658.44\n",
            "VAE Label Entropy:  2.275  (max=2.303)\n",
            "VAE KL Divergence:  0.0302\n",
            "VAE Sampling speed: 28577.4 imgs/s  |  5k imgs in 0.2s\n",
            "  [saved] ./outputs/vae_label_dist.png\n",
            "Generator params:     3,017,600\n",
            "Discriminator params: 2,757,376\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1398/3455737240.py:447: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler_G  = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == 'cuda'))\n",
            "/tmp/ipython-input-1398/3455737240.py:448: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler_D  = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == 'cuda'))\n",
            "/tmp/ipython-input-1398/3455737240.py:468: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(DEVICE.type == 'cuda')):\n",
            "/tmp/ipython-input-1398/3455737240.py:476: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(DEVICE.type == 'cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GAN ep  1  G=3.9623  D=0.7006  [checkpoint]\n",
            "GAN ep 10  G=2.3674  D=0.8008  [checkpoint]\n",
            "GAN ep 20  G=2.7898  D=0.6534\n",
            "GAN ep 25  G=3.0080  D=0.5874  [checkpoint]\n",
            "GAN ep 30  G=3.1151  D=0.6188\n",
            "GAN ep 40  G=3.4216  D=0.5210\n",
            "GAN ep 50  G=3.5246  D=0.5289  [checkpoint]\n",
            "GAN training time: 15.9 min\n",
            "  [saved] ./outputs/gan_training_curves.png\n",
            "  [saved] ./outputs/gan_samples.png\n",
            "  [saved] ./outputs/gan_interpolation.png\n",
            "  [saved] ./outputs/gan_mode_collapse_watch.png\n",
            "Evaluating GAN...\n",
            "  5000 images  →  11226.9 imgs/s  (0.4s total)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1398/3455737240.py:187: LinAlgWarning: Matrix is singular. The result might be inaccurate or the array might not have a square root.\n",
            "  covmean = sqrtm(sig1 @ sig2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GAN cFID:           94.07\n",
            "GAN Label Entropy:  2.287\n",
            "GAN KL Divergence:  0.0153\n",
            "GAN Sampling speed: 11226.9 imgs/s  |  5k imgs in 0.4s\n",
            "  [saved] ./outputs/gan_label_dist.png\n",
            "U-Net parameters: 5,826,817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1398/3455737240.py:633: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler_diff = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == 'cuda'))\n",
            "/tmp/ipython-input-1398/3455737240.py:645: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(DEVICE.type == 'cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DDPM ep  1  MSE=0.09943\n",
            "DDPM ep  5  MSE=0.04598\n",
            "DDPM ep 10  MSE=0.04214\n",
            "DDPM ep 15  MSE=0.04083\n",
            "DDPM ep 20  MSE=0.03930\n",
            "DDPM ep 25  MSE=0.03879\n",
            "DDPM ep 30  MSE=0.03786\n",
            "Diffusion training time: 25.0 min\n",
            "  [saved] ./outputs/diff_training_curve.png\n",
            "\n",
            "Sampling 500 steps...\n",
            "  [saved] ./outputs/diff_samples_500steps.png\n",
            "\n",
            "Sampling 200 steps...\n",
            "  [saved] ./outputs/diff_samples_200steps.png\n",
            "\n",
            "Sampling 100 steps...\n",
            "  [saved] ./outputs/diff_samples_100steps.png\n",
            "\n",
            "Sampling 50 steps...\n",
            "  [saved] ./outputs/diff_samples_50steps.png\n",
            "\n",
            "Sampling 20 steps...\n",
            "  [saved] ./outputs/diff_samples_20steps.png\n",
            "\n",
            "--- Diffusion @500 steps ---\n",
            "  5000 images  →  3.2 imgs/s  (1561.2s total)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1398/3455737240.py:187: LinAlgWarning: Matrix is singular. The result might be inaccurate or the array might not have a square root.\n",
            "  covmean = sqrtm(sig1 @ sig2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  cFID=128.30  entropy=2.282  KL=0.0275  speed=3.2 imgs/s\n",
            "  [saved] ./outputs/diff_label_dist_500steps.png\n",
            "\n",
            "--- Diffusion @200 steps ---\n",
            "  5000 images  →  8.0 imgs/s  (622.5s total)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1398/3455737240.py:187: LinAlgWarning: Matrix is singular. The result might be inaccurate or the array might not have a square root.\n",
            "  covmean = sqrtm(sig1 @ sig2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  cFID=3128.08  entropy=0.741  KL=6.6262  speed=8.0 imgs/s\n",
            "  [saved] ./outputs/diff_label_dist_200steps.png\n",
            "\n",
            "--- Diffusion @100 steps ---\n",
            "  5000 images  →  16.0 imgs/s  (312.1s total)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1398/3455737240.py:187: LinAlgWarning: Matrix is singular. The result might be inaccurate or the array might not have a square root.\n",
            "  covmean = sqrtm(sig1 @ sig2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  cFID=3073.25  entropy=0.561  KL=7.8581  speed=16.0 imgs/s\n",
            "  [saved] ./outputs/diff_label_dist_100steps.png\n",
            "\n",
            "--- Diffusion @50 steps ---\n",
            "  5000 images  →  31.9 imgs/s  (156.7s total)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1398/3455737240.py:187: LinAlgWarning: Matrix is singular. The result might be inaccurate or the array might not have a square root.\n",
            "  covmean = sqrtm(sig1 @ sig2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  cFID=3039.89  entropy=0.550  KL=7.8337  speed=31.9 imgs/s\n",
            "  [saved] ./outputs/diff_label_dist_50steps.png\n",
            "\n",
            "--- Diffusion @20 steps ---\n",
            "  5000 images  →  80.0 imgs/s  (62.5s total)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1398/3455737240.py:187: LinAlgWarning: Matrix is singular. The result might be inaccurate or the array might not have a square root.\n",
            "  covmean = sqrtm(sig1 @ sig2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  cFID=3026.39  entropy=0.558  KL=7.7136  speed=80.0 imgs/s\n",
            "  [saved] ./outputs/diff_label_dist_20steps.png\n",
            "\n",
            "               Model    cFID  Entropy  KL Div  Imgs/sec  Time 5k (s)  # Params  Train time (min)\n",
            "                 VAE  658.44    2.275  0.0302   28577.4          0.2 2,626,817               8.5\n",
            "                 GAN   94.07    2.287  0.0153   11226.9          0.4 5,774,976              15.9\n",
            "Diffusion (500steps)  128.30    2.282  0.0275       3.2       1561.2 5,826,817              25.0\n",
            "Diffusion (200steps) 3128.08    0.741  6.6262       8.0        622.5 5,826,817              25.0\n",
            "Diffusion (100steps) 3073.25    0.561  7.8581      16.0        312.1 5,826,817              25.0\n",
            " Diffusion (50steps) 3039.89    0.550  7.8337      31.9        156.7 5,826,817              25.0\n",
            " Diffusion (20steps) 3026.39    0.558  7.7136      80.0         62.5 5,826,817              25.0\n",
            "  [saved] ./outputs/results_summary.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1398/3455737240.py:768: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_xticklabels(models, rotation=30, ha='right', fontsize=8)\n",
            "/tmp/ipython-input-1398/3455737240.py:768: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_xticklabels(models, rotation=30, ha='right', fontsize=8)\n",
            "/tmp/ipython-input-1398/3455737240.py:768: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_xticklabels(models, rotation=30, ha='right', fontsize=8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [saved] ./outputs/trilemma_comparison.png\n",
            "  [saved] ./outputs/diff_speed_quality_tradeoff.png\n",
            "\n",
            "============================================================\n",
            "ALL SAVED FILES IN /content/outputs\n",
            "============================================================\n",
            "  diff_label_dist_100steps.png                      41.2 KB\n",
            "  diff_label_dist_200steps.png                      40.3 KB\n",
            "  diff_label_dist_20steps.png                       41.5 KB\n",
            "  diff_label_dist_500steps.png                      45.6 KB\n",
            "  diff_label_dist_50steps.png                       40.9 KB\n",
            "  diff_samples_100steps.png                        375.7 KB\n",
            "  diff_samples_200steps.png                        361.4 KB\n",
            "  diff_samples_20steps.png                         372.6 KB\n",
            "  diff_samples_500steps.png                        174.7 KB\n",
            "  diff_samples_50steps.png                         374.6 KB\n",
            "  diff_speed_quality_tradeoff.png                   56.6 KB\n",
            "  diff_training_curve.png                           34.1 KB\n",
            "  gan_interpolation.png                             26.3 KB\n",
            "  gan_label_dist.png                                39.9 KB\n",
            "  gan_mode_collapse_watch.png                       49.3 KB\n",
            "  gan_samples.png                                  162.4 KB\n",
            "  gan_training_curves.png                           46.4 KB\n",
            "  results_summary.csv                                0.5 KB\n",
            "  trilemma_comparison.png                           59.3 KB\n",
            "  vae_interpolation.png                             26.2 KB\n",
            "  vae_label_dist.png                                43.9 KB\n",
            "  vae_reconstructions.png                          160.2 KB\n",
            "  vae_samples.png                                  167.2 KB\n",
            "  vae_training_curves.png                           79.0 KB\n",
            "\n",
            "=== TRILEMMA ANALYSIS ===\n",
            "\n",
            "1. SAMPLE QUALITY (cFID ↓)\n",
            "   GAN usually achieves the sharpest samples and lowest cFID due to adversarial training.\n",
            "   Diffusion at full steps is competitive but slower to generate.\n",
            "   VAE produces blurrier samples (pixel-MSE objective), giving a higher cFID.\n",
            "\n",
            "2. DIVERSITY / COVERAGE (Entropy ↑, KL ↓)\n",
            "   VAE has the best coverage — it reconstructs across the full training distribution.\n",
            "   GAN may collapse to a subset of modes, especially in early training (visible in histograms).\n",
            "   Diffusion achieves good coverage when given sufficient denoising steps.\n",
            "\n",
            "3. SAMPLING SPEED (imgs/sec ↑)\n",
            "   GAN and VAE are single forward-pass models — orders of magnitude faster than diffusion.\n",
            "   Diffusion with 20–50 steps approaches a practical speed while keeping quality reasonable.\n",
            "\n",
            "FAILURE MODES\n",
            "   VAE  : Blur from Gaussian decoder / pixel-MSE loss.\n",
            "   GAN  : Training instability; early mode collapse visible in checkpoint label histograms.\n",
            "   Diff : Slow; quality noticeably degrades below ~50 steps.\n",
            "\n",
            "SUMMARY — The Generative Trilemma\n",
            "   No single model dominates all three axes simultaneously:\n",
            "   • VAE   → fastest + most diverse, but lowest quality.\n",
            "   • GAN   → fastest + sharpest,     but diversity risk.\n",
            "   • Diff  → best quality + coverage, but slowest without step reduction.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# # CSIT 595 Assignment 1 — Generative Models on Fashion-MNIST\n",
        "# **VAE · GAN · Diffusion Model (DDPM)**\n",
        "#\n",
        "# Run on Google Colab with GPU runtime.\n",
        "# All output images and graphs are saved to ./outputs/\n",
        "\n",
        "# ============================================================\n",
        "# 0. Imports & Setup\n",
        "# ============================================================\n",
        "\n",
        "import os, time, math, copy\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')          # Non-interactive backend — saves files without display\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from scipy.linalg import sqrtm\n",
        "import pandas as pd\n",
        "\n",
        "# ── Output directory\n",
        "OUTPUT_DIR = './outputs'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f'All images/graphs will be saved to: {os.path.abspath(OUTPUT_DIR)}')\n",
        "\n",
        "# ── GPU setup\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {DEVICE}')\n",
        "if DEVICE.type == 'cuda':\n",
        "    print(f'  GPU: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'  VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
        "    # Speed-up flags\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# ── Hyperparameters\n",
        "BATCH_SIZE   = 128\n",
        "IMG_SIZE     = 32\n",
        "NC           = 1\n",
        "LATENT_DIM   = 128\n",
        "VAE_EPOCHS   = 30\n",
        "GAN_EPOCHS   = 50\n",
        "DIFF_STEPS   = 500\n",
        "DIFF_EPOCHS  = 30\n",
        "EVAL_SAMPLES = 5000\n",
        "NUM_CLASSES  = 10\n",
        "\n",
        "CLASS_NAMES = ['T-shirt','Trouser','Pullover','Dress','Coat',\n",
        "               'Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
        "print('Setup complete.')\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Helper: save figure util\n",
        "# ============================================================\n",
        "\n",
        "def savefig(filename, dpi=150):\n",
        "    \"\"\"Save current matplotlib figure to OUTPUT_DIR and close it.\"\"\"\n",
        "    path = os.path.join(OUTPUT_DIR, filename)\n",
        "    plt.savefig(path, dpi=dpi, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f'  [saved] {path}')\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1. Common Setup & Evaluator Network\n",
        "# ============================================================\n",
        "\n",
        "# ── DataLoaders\n",
        "num_workers = 4 if DEVICE.type == 'cuda' else 2\n",
        "transform = transforms.Compose([\n",
        "    transforms.Pad(2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "train_ds = datasets.FashionMNIST('./data', train=True,  download=True, transform=transform)\n",
        "test_ds  = datasets.FashionMNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "    num_workers=num_workers, pin_memory=(DEVICE.type == 'cuda'),\n",
        "    persistent_workers=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "    num_workers=num_workers, pin_memory=(DEVICE.type == 'cuda'),\n",
        "    persistent_workers=True\n",
        ")\n",
        "print(f'Train: {len(train_ds):,}  |  Test: {len(test_ds):,}')\n",
        "\n",
        "\n",
        "# ── Evaluator CNN\n",
        "class EvaluatorCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1,  32, 3, padding=1), nn.BatchNorm2d(32),  nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64),  nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "            nn.Conv2d(128,256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(256,256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d(4),\n",
        "        )\n",
        "        self.penultimate = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256*4*4, 512), nn.ReLU(), nn.Dropout(0.4),\n",
        "        )\n",
        "        self.classifier = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        feat = self.features(x)\n",
        "        feat = self.penultimate(feat)\n",
        "        if return_features:\n",
        "            return feat\n",
        "        return self.classifier(feat)\n",
        "\n",
        "\n",
        "def train_evaluator(model, train_loader, test_loader, epochs=15):\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, epochs)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == 'cuda'))\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total_loss = correct = total = 0\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
        "            opt.zero_grad()\n",
        "            with torch.cuda.amp.autocast(enabled=(DEVICE.type == 'cuda')):\n",
        "                logits = model(x)\n",
        "                loss   = F.cross_entropy(logits, y)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            correct    += (logits.argmax(1) == y).sum().item()\n",
        "            total      += x.size(0)\n",
        "        sched.step()\n",
        "        if ep % 5 == 0 or ep == 1:\n",
        "            print(f'  Epoch {ep:3d}  loss={total_loss/total:.4f}  acc={correct/total*100:.1f}%')\n",
        "\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
        "            correct += (model(x).argmax(1) == y).sum().item()\n",
        "            total   += x.size(0)\n",
        "    print(f'\\n✓ Evaluator Test Accuracy: {correct/total*100:.2f}%')\n",
        "    return model\n",
        "\n",
        "\n",
        "print('Training Evaluator CNN...')\n",
        "evaluator = EvaluatorCNN().to(DEVICE)\n",
        "evaluator = train_evaluator(evaluator, train_loader, test_loader, epochs=15)\n",
        "torch.save(evaluator.state_dict(), 'evaluator.pth')\n",
        "\n",
        "\n",
        "# ── Evaluation utilities\n",
        "\n",
        "def get_features_and_labels(model, images_tensor, batch_size=256):\n",
        "    model.eval()\n",
        "    feats, preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(images_tensor), batch_size):\n",
        "            xb = images_tensor[i:i+batch_size].to(DEVICE, non_blocking=True)\n",
        "            feats.append(model(xb, return_features=True).cpu())\n",
        "            preds.append(model(xb).argmax(1).cpu())\n",
        "    return torch.cat(feats).numpy(), torch.cat(preds).numpy()\n",
        "\n",
        "\n",
        "def compute_cfid(real_feats, gen_feats):\n",
        "    mu1, sig1 = real_feats.mean(0), np.cov(real_feats, rowvar=False)\n",
        "    mu2, sig2 = gen_feats.mean(0),  np.cov(gen_feats,  rowvar=False)\n",
        "    diff    = mu1 - mu2\n",
        "    covmean = sqrtm(sig1 @ sig2)\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "    return float(diff @ diff + np.trace(sig1 + sig2 - 2*covmean))\n",
        "\n",
        "\n",
        "def coverage_metrics(pred_labels, real_label_dist=None):\n",
        "    counts = np.bincount(pred_labels, minlength=NUM_CLASSES).astype(float)\n",
        "    p      = counts / counts.sum()\n",
        "    entropy = -np.sum(p * np.log(p + 1e-8))\n",
        "    kl = None\n",
        "    if real_label_dist is not None:\n",
        "        kl = np.sum(real_label_dist * np.log((real_label_dist+1e-8)/(p+1e-8)))\n",
        "    return p, entropy, kl\n",
        "\n",
        "\n",
        "def measure_sampling_speed(sample_fn, n=EVAL_SAMPLES, batch=256):\n",
        "    all_imgs, remaining = [], n\n",
        "    t0 = time.time()\n",
        "    while remaining > 0:\n",
        "        bs = min(batch, remaining)\n",
        "        all_imgs.append(sample_fn(bs).cpu())\n",
        "        remaining -= bs\n",
        "    elapsed = time.time() - t0\n",
        "    speed = n / elapsed\n",
        "    print(f'  {n} images  →  {speed:.1f} imgs/s  ({elapsed:.1f}s total)')\n",
        "    return torch.cat(all_imgs), elapsed, speed\n",
        "\n",
        "\n",
        "def save_grid(imgs, title='', filename='grid.png', nrow=10, figsize=(12, 12)):\n",
        "    \"\"\"Save an image grid to file.\"\"\"\n",
        "    imgs = (imgs.clamp(-1, 1) + 1) / 2\n",
        "    grid = torchvision.utils.make_grid(imgs[:nrow*nrow], nrow=nrow, padding=2)\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.imshow(grid.permute(1, 2, 0).squeeze(), cmap='gray')\n",
        "    plt.title(title, fontsize=13)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    savefig(filename)\n",
        "\n",
        "\n",
        "def save_label_hist(dist, title='Label Distribution', filename='label_hist.png'):\n",
        "    plt.figure(figsize=(8, 3))\n",
        "    plt.bar(range(NUM_CLASSES), dist)\n",
        "    plt.xticks(range(NUM_CLASSES), CLASS_NAMES, rotation=45, ha='right')\n",
        "    plt.ylabel('Proportion')\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    savefig(filename)\n",
        "\n",
        "\n",
        "# Pre-compute real features\n",
        "print('Extracting real features for cFID baseline...')\n",
        "real_imgs_all = torch.stack([train_ds[i][0] for i in range(EVAL_SAMPLES)])\n",
        "real_feats, real_labels = get_features_and_labels(evaluator, real_imgs_all)\n",
        "REAL_LABEL_DIST = np.bincount(real_labels, minlength=NUM_CLASSES).astype(float)\n",
        "REAL_LABEL_DIST /= REAL_LABEL_DIST.sum()\n",
        "print('Done.')\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2. Variational Autoencoder (VAE)\n",
        "# ============================================================\n",
        "\n",
        "class VAEEncoder(nn.Module):\n",
        "    def __init__(self, latent_dim=LATENT_DIM):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1,   32, 4, 2, 1), nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(32,  64, 4, 2, 1), nn.BatchNorm2d(64),  nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(128,256, 4, 2, 1), nn.BatchNorm2d(256), nn.LeakyReLU(0.2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256*2*2, 512), nn.ReLU(),\n",
        "        )\n",
        "        self.fc_mu  = nn.Linear(512, latent_dim)\n",
        "        self.fc_log = nn.Linear(512, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.net(x)\n",
        "        return self.fc_mu(h), self.fc_log(h)\n",
        "\n",
        "\n",
        "class VAEDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim=LATENT_DIM):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 512), nn.ReLU(),\n",
        "            nn.Linear(512, 256*2*2),    nn.ReLU()\n",
        "        )\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128,  64, 4, 2, 1), nn.BatchNorm2d(64),  nn.ReLU(),\n",
        "            nn.ConvTranspose2d( 64,  32, 4, 2, 1), nn.BatchNorm2d(32),  nn.ReLU(),\n",
        "            nn.ConvTranspose2d( 32,   1, 4, 2, 1), nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.net(self.fc(z).view(-1, 256, 2, 2))\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim=LATENT_DIM):\n",
        "        super().__init__()\n",
        "        self.encoder = VAEEncoder(latent_dim)\n",
        "        self.decoder = VAEDecoder(latent_dim)\n",
        "\n",
        "    def reparameterise(self, mu, log_var):\n",
        "        return mu + (0.5 * log_var).exp() * torch.randn_like(mu)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encoder(x)\n",
        "        return self.decoder(self.reparameterise(mu, log_var)), mu, log_var\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, n):\n",
        "        return self.decoder(torch.randn(n, LATENT_DIM, device=DEVICE))\n",
        "\n",
        "\n",
        "def vae_loss(recon, x, mu, log_var, beta=1.0):\n",
        "    r = F.mse_loss(recon, x, reduction='sum') / x.size(0)\n",
        "    k = -0.5 * (1 + log_var - mu.pow(2) - log_var.exp()).sum(1).mean()\n",
        "    return r + beta*k, r, k\n",
        "\n",
        "\n",
        "print(f'VAE parameters: {sum(p.numel() for p in VAE().parameters()):,}')\n",
        "\n",
        "vae = VAE().to(DEVICE)\n",
        "opt_vae    = torch.optim.Adam(vae.parameters(), lr=2e-4)\n",
        "sched_vae  = torch.optim.lr_scheduler.CosineAnnealingLR(opt_vae, VAE_EPOCHS)\n",
        "scaler_vae = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == 'cuda'))\n",
        "\n",
        "hist_vae = {'total': [], 'recon': [], 'kl': []}\n",
        "t0_vae   = time.time()\n",
        "\n",
        "for ep in range(1, VAE_EPOCHS+1):\n",
        "    vae.train()\n",
        "    ep_t = ep_r = ep_k = n = 0\n",
        "    for x, _ in train_loader:\n",
        "        x = x.to(DEVICE, non_blocking=True)\n",
        "        opt_vae.zero_grad()\n",
        "        with torch.cuda.amp.autocast(enabled=(DEVICE.type == 'cuda')):\n",
        "            recon, mu, lv = vae(x)\n",
        "            loss, r, k    = vae_loss(recon, x, mu, lv)\n",
        "        scaler_vae.scale(loss).backward()\n",
        "        scaler_vae.step(opt_vae)\n",
        "        scaler_vae.update()\n",
        "        ep_t += loss.item(); ep_r += r.item(); ep_k += k.item(); n += 1\n",
        "    sched_vae.step()\n",
        "    hist_vae['total'].append(ep_t/n)\n",
        "    hist_vae['recon'].append(ep_r/n)\n",
        "    hist_vae['kl'].append(ep_k/n)\n",
        "    if ep % 5 == 0 or ep == 1:\n",
        "        print(f'VAE ep{ep:3d}  total={ep_t/n:.3f}  recon={ep_r/n:.3f}  kl={ep_k/n:.3f}')\n",
        "\n",
        "vae_train_time = time.time() - t0_vae\n",
        "print(f'VAE training time: {vae_train_time/60:.1f} min')\n",
        "torch.save(vae.state_dict(), 'vae.pth')\n",
        "\n",
        "# ── Training curves\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
        "for ax, key, c in zip(axes, ['total','recon','kl'], ['tab:blue','tab:orange','tab:green']):\n",
        "    ax.plot(hist_vae[key], color=c)\n",
        "    ax.set_title(f'VAE {key.capitalize()} Loss')\n",
        "    ax.set_xlabel('Epoch'); ax.set_ylabel('Loss')\n",
        "plt.suptitle('VAE Training Curves', fontsize=14)\n",
        "plt.tight_layout()\n",
        "savefig('vae_training_curves.png')\n",
        "\n",
        "# ── Reconstructions\n",
        "vae.eval()\n",
        "with torch.no_grad():\n",
        "    real_batch          = next(iter(test_loader))[0][:50].to(DEVICE)\n",
        "    recon_batch, _, _   = vae(real_batch)\n",
        "\n",
        "interleaved = torch.stack([t for pair in zip(real_batch.cpu(), recon_batch.cpu()) for t in pair])\n",
        "save_grid(interleaved, 'VAE Reconstructions (original | reconstruction pairs)',\n",
        "          'vae_reconstructions.png', nrow=10)\n",
        "\n",
        "# ── Unconditional samples\n",
        "samples_vae = vae.sample(100).cpu()\n",
        "save_grid(samples_vae, 'VAE — 100 Unconditional Samples', 'vae_samples.png', nrow=10)\n",
        "\n",
        "# ── Latent interpolation\n",
        "with torch.no_grad():\n",
        "    x1 = test_ds[0][0].unsqueeze(0).to(DEVICE)\n",
        "    x2 = test_ds[1][0].unsqueeze(0).to(DEVICE)\n",
        "    mu1, _ = vae.encoder(x1)\n",
        "    mu2, _ = vae.encoder(x2)\n",
        "    alphas  = torch.linspace(0, 1, 10).to(DEVICE)\n",
        "    interp_vae = torch.cat([vae.decoder(mu1*(1-a)+mu2*a) for a in alphas]).cpu()\n",
        "save_grid(interp_vae, 'VAE Latent Interpolation (10 steps)', 'vae_interpolation.png', nrow=10)\n",
        "\n",
        "# ── Evaluate VAE\n",
        "print('Evaluating VAE...')\n",
        "vae_imgs, vae_elapsed, vae_speed = measure_sampling_speed(vae.sample)\n",
        "vae_gen_feats, vae_gen_labels    = get_features_and_labels(evaluator, vae_imgs)\n",
        "vae_cfid                          = compute_cfid(real_feats, vae_gen_feats)\n",
        "vae_dist, vae_ent, vae_kl        = coverage_metrics(vae_gen_labels, REAL_LABEL_DIST)\n",
        "\n",
        "print(f'VAE cFID:           {vae_cfid:.2f}')\n",
        "print(f'VAE Label Entropy:  {vae_ent:.3f}  (max={math.log(10):.3f})')\n",
        "print(f'VAE KL Divergence:  {vae_kl:.4f}')\n",
        "print(f'VAE Sampling speed: {vae_speed:.1f} imgs/s  |  5k imgs in {vae_elapsed:.1f}s')\n",
        "save_label_hist(vae_dist, 'VAE — Generated Label Distribution', 'vae_label_dist.png')\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3. Generative Adversarial Network (DCGAN)\n",
        "# ============================================================\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "        nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "    if isinstance(m, nn.BatchNorm2d):\n",
        "        nn.init.normal_(m.weight, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim=LATENT_DIM):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(z_dim, 512, 2, 1, 0, bias=False), nn.BatchNorm2d(512), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(512,  256, 4, 2, 1, bias=False), nn.BatchNorm2d(256), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(256,  128, 4, 2, 1, bias=False), nn.BatchNorm2d(128), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128,   64, 4, 2, 1, bias=False), nn.BatchNorm2d(64),  nn.ReLU(True),\n",
        "            nn.ConvTranspose2d( 64,    1, 4, 2, 1, bias=False), nn.Tanh(),\n",
        "        )\n",
        "    def forward(self, z):\n",
        "        return self.net(z.view(-1, LATENT_DIM, 1, 1))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, n):\n",
        "        return self(torch.randn(n, LATENT_DIM, device=DEVICE))\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(  1,  64, 4, 2, 1, bias=False), nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d( 64, 128, 4, 2, 1, bias=False), nn.BatchNorm2d(128), nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias=False), nn.BatchNorm2d(256), nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias=False), nn.BatchNorm2d(512), nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(512,   1, 2, 1, 0, bias=False), nn.Flatten(),\n",
        "            # No Sigmoid here — BCEWithLogitsLoss applies it internally (autocast-safe)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "G = Generator().to(DEVICE).apply(weights_init)\n",
        "D = Discriminator().to(DEVICE).apply(weights_init)\n",
        "print(f'Generator params:     {sum(p.numel() for p in G.parameters()):,}')\n",
        "print(f'Discriminator params: {sum(p.numel() for p in D.parameters()):,}')\n",
        "\n",
        "opt_G     = torch.optim.Adam(G.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "opt_D     = torch.optim.Adam(D.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "criterion = nn.BCEWithLogitsLoss()   # autocast-safe; no Sigmoid needed in D\n",
        "scaler_G  = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == 'cuda'))\n",
        "scaler_D  = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == 'cuda'))\n",
        "\n",
        "fixed_noise   = torch.randn(100, LATENT_DIM, device=DEVICE)\n",
        "hist_gan      = {'G': [], 'D': []}\n",
        "gan_checkpoints = {}\n",
        "CKPT_EPOCHS   = [1, 10, 25, GAN_EPOCHS]\n",
        "\n",
        "t0_gan = time.time()\n",
        "\n",
        "for ep in range(1, GAN_EPOCHS+1):\n",
        "    G.train(); D.train()\n",
        "    ep_G = ep_D = n = 0\n",
        "    for x, _ in train_loader:\n",
        "        bs   = x.size(0)\n",
        "        real = x.to(DEVICE, non_blocking=True)\n",
        "        rl   = torch.full((bs, 1), 0.9, device=DEVICE)\n",
        "        fl   = torch.zeros(bs, 1, device=DEVICE)\n",
        "\n",
        "        # ── Train D\n",
        "        opt_D.zero_grad()\n",
        "        with torch.cuda.amp.autocast(enabled=(DEVICE.type == 'cuda')):\n",
        "            z     = torch.randn(bs, LATENT_DIM, device=DEVICE)\n",
        "            lossD = criterion(D(real), rl) + criterion(D(G(z).detach()), fl)\n",
        "        scaler_D.scale(lossD).backward()\n",
        "        scaler_D.step(opt_D); scaler_D.update()\n",
        "\n",
        "        # ── Train G\n",
        "        opt_G.zero_grad()\n",
        "        with torch.cuda.amp.autocast(enabled=(DEVICE.type == 'cuda')):\n",
        "            z     = torch.randn(bs, LATENT_DIM, device=DEVICE)\n",
        "            lossG = criterion(D(G(z)), torch.ones(bs, 1, device=DEVICE))\n",
        "        scaler_G.scale(lossG).backward()\n",
        "        scaler_G.step(opt_G); scaler_G.update()\n",
        "\n",
        "        ep_G += lossG.item(); ep_D += lossD.item(); n += 1\n",
        "\n",
        "    hist_gan['G'].append(ep_G/n)\n",
        "    hist_gan['D'].append(ep_D/n)\n",
        "\n",
        "    if ep in CKPT_EPOCHS:\n",
        "        G.eval()\n",
        "        with torch.no_grad():\n",
        "            ck = torch.cat([G(torch.randn(256, LATENT_DIM, device=DEVICE)).cpu() for _ in range(20)])\n",
        "        _, ck_lbl = get_features_and_labels(evaluator, ck)\n",
        "        d = np.bincount(ck_lbl, minlength=NUM_CLASSES).astype(float)\n",
        "        gan_checkpoints[ep] = d / d.sum()\n",
        "        print(f'GAN ep{ep:3d}  G={ep_G/n:.4f}  D={ep_D/n:.4f}  [checkpoint]')\n",
        "    elif ep % 10 == 0:\n",
        "        print(f'GAN ep{ep:3d}  G={ep_G/n:.4f}  D={ep_D/n:.4f}')\n",
        "\n",
        "gan_train_time = time.time() - t0_gan\n",
        "print(f'GAN training time: {gan_train_time/60:.1f} min')\n",
        "torch.save(G.state_dict(), 'generator.pth')\n",
        "\n",
        "# ── Training curves\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(hist_gan['G'], label='Generator')\n",
        "plt.plot(hist_gan['D'], label='Discriminator')\n",
        "plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
        "plt.title('GAN Training Curves'); plt.legend()\n",
        "plt.tight_layout()\n",
        "savefig('gan_training_curves.png')\n",
        "\n",
        "# ── Unconditional samples\n",
        "G.eval()\n",
        "with torch.no_grad():\n",
        "    gan_samples = G(fixed_noise).cpu()\n",
        "save_grid(gan_samples, 'GAN — 100 Unconditional Samples', 'gan_samples.png', nrow=10)\n",
        "\n",
        "# ── Latent interpolation\n",
        "with torch.no_grad():\n",
        "    z1 = torch.randn(1, LATENT_DIM, device=DEVICE)\n",
        "    z2 = torch.randn(1, LATENT_DIM, device=DEVICE)\n",
        "    alphas     = torch.linspace(0, 1, 10).to(DEVICE)\n",
        "    interp_gan = torch.cat([G(z1*(1-a)+z2*a) for a in alphas]).cpu()\n",
        "save_grid(interp_gan, 'GAN Latent Interpolation (10 steps)', 'gan_interpolation.png', nrow=10)\n",
        "\n",
        "# ── Mode collapse watch\n",
        "fig, axes = plt.subplots(1, len(gan_checkpoints), figsize=(14, 3))\n",
        "for ax, (ep, dist) in zip(axes, gan_checkpoints.items()):\n",
        "    ax.bar(range(NUM_CLASSES), dist)\n",
        "    ax.set_title(f'Epoch {ep}')\n",
        "    ax.set_xticks(range(NUM_CLASSES))\n",
        "    ax.set_xticklabels([c[:4] for c in CLASS_NAMES], rotation=45, ha='right', fontsize=7)\n",
        "plt.suptitle('GAN Mode Collapse Watch — Label Histograms at Checkpoints', fontsize=12)\n",
        "plt.tight_layout()\n",
        "savefig('gan_mode_collapse_watch.png')\n",
        "\n",
        "# ── Evaluate GAN\n",
        "print('Evaluating GAN...')\n",
        "gan_imgs, gan_elapsed, gan_speed = measure_sampling_speed(G.sample)\n",
        "gan_gen_feats, gan_gen_labels    = get_features_and_labels(evaluator, gan_imgs)\n",
        "gan_cfid                          = compute_cfid(real_feats, gan_gen_feats)\n",
        "gan_dist, gan_ent, gan_kl        = coverage_metrics(gan_gen_labels, REAL_LABEL_DIST)\n",
        "\n",
        "print(f'GAN cFID:           {gan_cfid:.2f}')\n",
        "print(f'GAN Label Entropy:  {gan_ent:.3f}')\n",
        "print(f'GAN KL Divergence:  {gan_kl:.4f}')\n",
        "print(f'GAN Sampling speed: {gan_speed:.1f} imgs/s  |  5k imgs in {gan_elapsed:.1f}s')\n",
        "save_label_hist(gan_dist, 'GAN — Generated Label Distribution', 'gan_label_dist.png')\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4. Diffusion Model (DDPM-style U-Net)\n",
        "# ============================================================\n",
        "\n",
        "# ── Noise schedule\n",
        "T         = DIFF_STEPS\n",
        "betas     = torch.linspace(1e-4, 0.02, T).to(DEVICE)\n",
        "alphas    = 1.0 - betas\n",
        "alpha_bar = torch.cumprod(alphas, dim=0)\n",
        "alpha_bar_prev = F.pad(alpha_bar[:-1], (1, 0), value=1.0)\n",
        "\n",
        "\n",
        "def q_sample(x0, t, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x0)\n",
        "    ab = alpha_bar[t].view(-1, 1, 1, 1)\n",
        "    return ab.sqrt()*x0 + (1-ab).sqrt()*noise, noise\n",
        "\n",
        "\n",
        "# ── U-Net with sinusoidal time embeddings\n",
        "\n",
        "class SinusoidalEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "    def forward(self, t):\n",
        "        half  = self.dim // 2\n",
        "        freqs = torch.exp(-math.log(10000) * torch.arange(half, device=t.device) / half)\n",
        "        emb   = t.float()[:, None] * freqs[None]\n",
        "        return torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, ch, t_dim=128):\n",
        "        super().__init__()\n",
        "        self.norm1  = nn.GroupNorm(8, ch)\n",
        "        self.conv1  = nn.Conv2d(ch, ch, 3, padding=1)\n",
        "        self.t_proj = nn.Linear(t_dim, ch)\n",
        "        self.norm2  = nn.GroupNorm(8, ch)\n",
        "        self.conv2  = nn.Conv2d(ch, ch, 3, padding=1)\n",
        "    def forward(self, x, te):\n",
        "        h = self.conv1(F.silu(self.norm1(x)))\n",
        "        h = h + self.t_proj(F.silu(te))[:, :, None, None]\n",
        "        return self.conv2(F.silu(self.norm2(h))) + x\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, ch=64, t_dim=128):\n",
        "        super().__init__()\n",
        "        self.t_emb = nn.Sequential(\n",
        "            SinusoidalEmb(t_dim),\n",
        "            nn.Linear(t_dim, t_dim), nn.SiLU(),\n",
        "            nn.Linear(t_dim, t_dim))\n",
        "        self.enc0  = nn.Conv2d(1, ch, 3, padding=1)\n",
        "        self.enc1  = ResBlock(ch, t_dim)\n",
        "        self.down1 = nn.Conv2d(ch, ch*2, 4, 2, 1)\n",
        "        self.enc2  = ResBlock(ch*2, t_dim)\n",
        "        self.down2 = nn.Conv2d(ch*2, ch*4, 4, 2, 1)\n",
        "        self.bot1  = ResBlock(ch*4, t_dim)\n",
        "        self.bot2  = ResBlock(ch*4, t_dim)\n",
        "        self.up1   = nn.ConvTranspose2d(ch*4, ch*2, 4, 2, 1)\n",
        "        self.dec1  = ResBlock(ch*4, t_dim)\n",
        "        self.up2   = nn.ConvTranspose2d(ch*4, ch, 4, 2, 1)\n",
        "        self.dec2  = ResBlock(ch*2, t_dim)\n",
        "        self.out   = nn.Sequential(\n",
        "            nn.GroupNorm(8, ch*2), nn.SiLU(),\n",
        "            nn.Conv2d(ch*2, 1, 3, padding=1))\n",
        "    def forward(self, x, t):\n",
        "        te = self.t_emb(t)\n",
        "        e0 = self.enc0(x)\n",
        "        e1 = self.enc1(e0, te)\n",
        "        e2 = self.enc2(self.down1(e1), te)\n",
        "        b  = self.bot2(self.bot1(self.down2(e2), te), te)\n",
        "        d1 = self.dec1(torch.cat([self.up1(b),  e2], 1), te)\n",
        "        d2 = self.dec2(torch.cat([self.up2(d1), e1], 1), te)\n",
        "        return self.out(d2)\n",
        "\n",
        "\n",
        "unet = UNet().to(DEVICE)\n",
        "print(f'U-Net parameters: {sum(p.numel() for p in unet.parameters()):,}')\n",
        "\n",
        "opt_diff    = torch.optim.Adam(unet.parameters(), lr=2e-4)\n",
        "sched_diff  = torch.optim.lr_scheduler.CosineAnnealingLR(opt_diff, DIFF_EPOCHS)\n",
        "scaler_diff = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == 'cuda'))\n",
        "\n",
        "hist_diff = []\n",
        "t0_diff   = time.time()\n",
        "\n",
        "for ep in range(1, DIFF_EPOCHS+1):\n",
        "    unet.train()\n",
        "    ep_loss = n = 0\n",
        "    for x, _ in train_loader:\n",
        "        x      = x.to(DEVICE, non_blocking=True)\n",
        "        t_rand = torch.randint(0, T, (x.size(0),), device=DEVICE)\n",
        "        opt_diff.zero_grad()\n",
        "        with torch.cuda.amp.autocast(enabled=(DEVICE.type == 'cuda')):\n",
        "            xt, noise = q_sample(x, t_rand)\n",
        "            loss      = F.mse_loss(unet(xt, t_rand), noise)\n",
        "        scaler_diff.scale(loss).backward()\n",
        "        scaler_diff.step(opt_diff); scaler_diff.update()\n",
        "        ep_loss += loss.item(); n += 1\n",
        "    sched_diff.step()\n",
        "    hist_diff.append(ep_loss/n)\n",
        "    if ep % 5 == 0 or ep == 1:\n",
        "        print(f'DDPM ep{ep:3d}  MSE={ep_loss/n:.5f}')\n",
        "\n",
        "diff_train_time = time.time() - t0_diff\n",
        "print(f'Diffusion training time: {diff_train_time/60:.1f} min')\n",
        "torch.save(unet.state_dict(), 'unet.pth')\n",
        "\n",
        "# ── Training curve\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(hist_diff)\n",
        "plt.xlabel('Epoch'); plt.ylabel('MSE')\n",
        "plt.title('Diffusion — Noise Prediction MSE')\n",
        "plt.tight_layout()\n",
        "savefig('diff_training_curve.png')\n",
        "\n",
        "\n",
        "# ── DDPM reverse sampling\n",
        "@torch.no_grad()\n",
        "def ddpm_sample(model, n, steps=T):\n",
        "    model.eval()\n",
        "    t_seq = torch.linspace(T-1, 0, steps, dtype=torch.long, device=DEVICE)\n",
        "    x     = torch.randn(n, 1, IMG_SIZE, IMG_SIZE, device=DEVICE)\n",
        "    for t_idx in t_seq:\n",
        "        tb       = t_idx.repeat(n)\n",
        "        eps_pred = model(x, tb)\n",
        "        ab_t     = alpha_bar[t_idx]\n",
        "        ab_tp    = alpha_bar_prev[t_idx]\n",
        "        beta_t   = betas[t_idx]\n",
        "        x0_pred  = ((x - (1-ab_t).sqrt()*eps_pred) / ab_t.sqrt()).clamp(-1, 1)\n",
        "        mean     = (ab_tp.sqrt()*beta_t/(1-ab_t))*x0_pred + \\\n",
        "                   (alphas[t_idx].sqrt()*(1-ab_tp)/(1-ab_t))*x\n",
        "        if t_idx > 0:\n",
        "            x = mean + (beta_t*(1-ab_tp)/(1-ab_t)).sqrt() * torch.randn_like(x)\n",
        "        else:\n",
        "            x = mean\n",
        "    return x\n",
        "\n",
        "\n",
        "# ── Sample at multiple step counts\n",
        "diff_results = {}\n",
        "for steps in [500, 200, 100, 50, 20]:\n",
        "    print(f'\\nSampling {steps} steps...')\n",
        "    t0   = time.time()\n",
        "    imgs = ddpm_sample(unet, 100, steps=steps).cpu()\n",
        "    spd  = 100 / (time.time() - t0)\n",
        "    save_grid(imgs, f'Diffusion — 100 samples @ {steps} steps  ({spd:.1f} imgs/s)',\n",
        "              f'diff_samples_{steps}steps.png', nrow=10)\n",
        "    diff_results[steps] = {'imgs': imgs, 'speed': spd}\n",
        "\n",
        "# ── Full evaluation at each step count\n",
        "diff_metrics = {}\n",
        "for steps in diff_results:\n",
        "    print(f'\\n--- Diffusion @{steps} steps ---')\n",
        "    fn = lambda n, s=steps: ddpm_sample(unet, n, steps=s)\n",
        "    gen_imgs, elapsed, spd = measure_sampling_speed(fn)\n",
        "    feats, labels          = get_features_and_labels(evaluator, gen_imgs)\n",
        "    cfid                   = compute_cfid(real_feats, feats)\n",
        "    dist, ent, kl          = coverage_metrics(labels, REAL_LABEL_DIST)\n",
        "    diff_metrics[steps]    = {'cfid': cfid, 'entropy': ent, 'kl': kl,\n",
        "                               'speed': spd, 'elapsed': elapsed, 'dist': dist}\n",
        "    print(f'  cFID={cfid:.2f}  entropy={ent:.3f}  KL={kl:.4f}  speed={spd:.1f} imgs/s')\n",
        "    save_label_hist(dist, f'Diffusion @{steps} steps — Label Distribution',\n",
        "                    f'diff_label_dist_{steps}steps.png')\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5. Performance Comparison & Trilemma Analysis\n",
        "# ============================================================\n",
        "\n",
        "vae_params  = sum(p.numel() for p in vae.parameters())\n",
        "gan_params  = sum(p.numel() for p in G.parameters()) + sum(p.numel() for p in D.parameters())\n",
        "diff_params = sum(p.numel() for p in unet.parameters())\n",
        "\n",
        "rows = [\n",
        "    {'Model': 'VAE',\n",
        "     'cFID': round(vae_cfid, 2), 'Entropy': round(vae_ent, 3),\n",
        "     'KL Div': round(vae_kl, 4), 'Imgs/sec': round(vae_speed, 1),\n",
        "     'Time 5k (s)': round(vae_elapsed, 1), '# Params': f'{vae_params:,}',\n",
        "     'Train time (min)': round(vae_train_time/60, 1)},\n",
        "    {'Model': 'GAN',\n",
        "     'cFID': round(gan_cfid, 2), 'Entropy': round(gan_ent, 3),\n",
        "     'KL Div': round(gan_kl, 4), 'Imgs/sec': round(gan_speed, 1),\n",
        "     'Time 5k (s)': round(gan_elapsed, 1), '# Params': f'{gan_params:,}',\n",
        "     'Train time (min)': round(gan_train_time/60, 1)},\n",
        "]\n",
        "for steps, m in diff_metrics.items():\n",
        "    rows.append({\n",
        "        'Model': f'Diffusion ({steps}steps)',\n",
        "        'cFID': round(m['cfid'], 2), 'Entropy': round(m['entropy'], 3),\n",
        "        'KL Div': round(m['kl'], 4), 'Imgs/sec': round(m['speed'], 1),\n",
        "        'Time 5k (s)': round(m['elapsed'], 1), '# Params': f'{diff_params:,}',\n",
        "        'Train time (min)': round(diff_train_time/60, 1)\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "print('\\n' + df.to_string(index=False))\n",
        "\n",
        "# Save summary table as CSV\n",
        "csv_path = os.path.join(OUTPUT_DIR, 'results_summary.csv')\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(f'  [saved] {csv_path}')\n",
        "\n",
        "# ── Bar charts for all three trilemma axes\n",
        "models    = ['VAE', 'GAN'] + [f'Diff-{s}' for s in diff_metrics]\n",
        "cfids     = [vae_cfid, gan_cfid]   + [m['cfid']    for m in diff_metrics.values()]\n",
        "entropies = [vae_ent,  gan_ent]    + [m['entropy'] for m in diff_metrics.values()]\n",
        "speeds    = [vae_speed, gan_speed] + [m['speed']   for m in diff_metrics.values()]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
        "for ax, vals, title, c in zip(axes,\n",
        "        [cfids, entropies, speeds],\n",
        "        ['cFID ↓', 'Entropy ↑', 'Sampling Speed ↑'],\n",
        "        ['salmon', 'steelblue', 'seagreen']):\n",
        "    ax.bar(models, vals, color=c)\n",
        "    ax.set_title(title, fontsize=12)\n",
        "    ax.set_xticklabels(models, rotation=30, ha='right', fontsize=8)\n",
        "plt.suptitle('Generative Trilemma Comparison', fontsize=14)\n",
        "plt.tight_layout()\n",
        "savefig('trilemma_comparison.png')\n",
        "\n",
        "# ── Diffusion speed-quality tradeoff\n",
        "sc  = list(diff_metrics.keys())\n",
        "fig, ax1 = plt.subplots(figsize=(8, 4))\n",
        "ax2 = ax1.twinx()\n",
        "ax1.plot(sc, [diff_metrics[s]['cfid']  for s in sc], 'o-', color='tomato',     label='cFID')\n",
        "ax2.plot(sc, [diff_metrics[s]['speed'] for s in sc], 's-', color='dodgerblue', label='Speed')\n",
        "ax1.set_xlabel('Diffusion Steps')\n",
        "ax1.set_ylabel('cFID ↓', color='tomato')\n",
        "ax2.set_ylabel('Imgs/sec ↑', color='dodgerblue')\n",
        "ax1.invert_xaxis()\n",
        "plt.title('Diffusion: Speed–Quality Tradeoff')\n",
        "fig.legend(loc='upper right', bbox_to_anchor=(0.85, 0.88))\n",
        "plt.tight_layout()\n",
        "savefig('diff_speed_quality_tradeoff.png')\n",
        "\n",
        "# ── Print final summary of saved files\n",
        "print('\\n' + '='*60)\n",
        "print('ALL SAVED FILES IN', os.path.abspath(OUTPUT_DIR))\n",
        "print('='*60)\n",
        "for f in sorted(os.listdir(OUTPUT_DIR)):\n",
        "    fpath = os.path.join(OUTPUT_DIR, f)\n",
        "    size  = os.path.getsize(fpath) / 1024\n",
        "    print(f'  {f:45s}  {size:7.1f} KB')\n",
        "\n",
        "print(\"\"\"\n",
        "=== TRILEMMA ANALYSIS ===\n",
        "\n",
        "1. SAMPLE QUALITY (cFID ↓)\n",
        "   GAN usually achieves the sharpest samples and lowest cFID due to adversarial training.\n",
        "   Diffusion at full steps is competitive but slower to generate.\n",
        "   VAE produces blurrier samples (pixel-MSE objective), giving a higher cFID.\n",
        "\n",
        "2. DIVERSITY / COVERAGE (Entropy ↑, KL ↓)\n",
        "   VAE has the best coverage — it reconstructs across the full training distribution.\n",
        "   GAN may collapse to a subset of modes, especially in early training (visible in histograms).\n",
        "   Diffusion achieves good coverage when given sufficient denoising steps.\n",
        "\n",
        "3. SAMPLING SPEED (imgs/sec ↑)\n",
        "   GAN and VAE are single forward-pass models — orders of magnitude faster than diffusion.\n",
        "   Diffusion with 20–50 steps approaches a practical speed while keeping quality reasonable.\n",
        "\n",
        "FAILURE MODES\n",
        "   VAE  : Blur from Gaussian decoder / pixel-MSE loss.\n",
        "   GAN  : Training instability; early mode collapse visible in checkpoint label histograms.\n",
        "   Diff : Slow; quality noticeably degrades below ~50 steps.\n",
        "\n",
        "SUMMARY — The Generative Trilemma\n",
        "   No single model dominates all three axes simultaneously:\n",
        "   • VAE   → fastest + most diverse, but lowest quality.\n",
        "   • GAN   → fastest + sharpest,     but diversity risk.\n",
        "   • Diff  → best quality + coverage, but slowest without step reduction.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/outputs.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "YivRPoxtFoF6",
        "outputId": "d3d96232-d462-4ebb-e15f-f0a8f38f0eff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c25b587d-f478-4da3-953c-cd3b5d6e7063\", \"outputs.zip\", 2305867)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}